{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# coding: utf-8\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import csv\n",
    "\n",
    "def fetch_content(url):\n",
    "    response = requests.get(url)\n",
    "    if response.status_code == 200:\n",
    "        return response.content\n",
    "    else:\n",
    "        print(f'Failed to retrieve {url}. Status code: {response.status_code}')\n",
    "        return None\n",
    "\n",
    "def save_to_csv(data, filename, headers=None):\n",
    "    with open(filename, 'w', newline='', encoding='utf-8') as csvfile:\n",
    "        writer = csv.writer(csvfile)\n",
    "        if headers:\n",
    "            writer.writerow(headers)\n",
    "        for row in data:\n",
    "            writer.writerow(row)\n",
    "\n",
    "\n",
    "def scrape_proxyway_images():\n",
    "    url = 'https://proxyway.com/news'\n",
    "    content = fetch_content(url)\n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        images_list = []\n",
    "        images = soup.select('img')\n",
    "        for image in images:\n",
    "            src = image.get('src')\n",
    "            alt = image.get('alt')\n",
    "            images_list.append([src, alt])\n",
    "        save_to_csv(images_list, 'data_gambar_yunita.csv', headers=['src', 'alt'])\n",
    "        for image in images_list:\n",
    "            print(image)\n",
    "\n",
    "def scrape_proxyway_subtitles():\n",
    "    url = 'https://proxyway.com/news'\n",
    "    content = fetch_content(url)\n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        paragraphs = soup.find_all('h2')\n",
    "        subtitles = [[paragraph.text] for paragraph in paragraphs]\n",
    "        save_to_csv(subtitles, 'subjudul_yunita.csv')\n",
    "        for subtitle in subtitles:\n",
    "            print(subtitle[0])\n",
    "\n",
    "def scrape_proxyway_descriptions():\n",
    "    url = 'https://proxyway.com/news'\n",
    "    content = fetch_content(url)\n",
    "    if content:\n",
    "        soup = BeautifulSoup(content, 'html.parser')\n",
    "        div_elements = soup.find_all('div', attrs={'data-widget_type': 'theme-post-excerpt.default'})\n",
    "        descriptions = []\n",
    "        for div_element in div_elements:\n",
    "            inner_div = div_element.find('div', class_='elementor-widget-container')\n",
    "            if inner_div:\n",
    "                text_content = inner_div.get_text(strip=True)\n",
    "                descriptions.append([text_content])\n",
    "                print(text_content)\n",
    "        save_to_csv(descriptions, 'keterangan_yunita.csv')\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "\n",
    "    \n",
    "    # Scrape proxyway.com/news for images and save to CSV\n",
    "    scrape_proxyway_images()\n",
    "    \n",
    "    # Scrape proxyway.com/news for subtitles (h2 tags) and save to CSV\n",
    "    scrape_proxyway_subtitles()\n",
    "    \n",
    "    # Scrape proxyway.com/news for specific div descriptions and save to CSV\n",
    "    scrape_proxyway_descriptions()\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
